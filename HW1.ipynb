{"cells":[{"cell_type":"markdown","metadata":{"id":"y86uNLRCqJ4H"},"source":["Overall guidelines:\n","- Only change the cells which say:\n","```# INSERT CODE/TEXT HERE```\n","- Except Q1, other content will be covered in class starting Jan 17.\n"]},{"cell_type":"markdown","metadata":{"id":"29sBMdraQzcn"},"source":["# Q1. Probability (1)"]},{"cell_type":"markdown","metadata":{"id":"YBqKC9_qzHDy"},"source":["Say we have three random variables $A$ and $B$ and $C$. Note that we’re using standard probability\n","theory notation where $P(A, B) = P(B, A)$, which simply means the joint probability of both A\n","and B occurring.\n","\n","### Q1.1\n","(0.5 point)\n","\n","Which of the following statements is always true?\n","1. $P(A|B) = P(B|A)$\n","2. $P(A, B) = P(A) + P(B)$\n","3. $P(A, B, C) = P(A)P(B)P(C)$\n","4. $P(A|B) = max(P(A),P(B))$\n","5. $P(A, B, C) = P(A)P(C)$\n","6. $P(A, B) = P(B|A)P(A)$\n","7. $P(A, B, C) = P(A)P(B|A)P(C|A, B)$\n","8. $P(A)=\\sum_{b ∈ \\text{domain}(B)} P(A, B=b)$\n","9. $P(A)=\\sum_{b ∈ \\text{domain}(B)} P(A|B=b)P(B=b)$\n","10.$\\log(1/P(A)) = 1 - log P(A)$"]},{"cell_type":"markdown","metadata":{"id":"gD6si1z2Izd6"},"source":["### Q1.2\n","(0.5 point)\n","\n","Now assume that A, B, and C are all independent of each other. Which of the above statements\n","are now true?"]},{"cell_type":"markdown","metadata":{"id":"BUxcuDifubco"},"source":["**\\# INSERT TEXT HERE**\n","\n","Answer the two subparts here by listing which statement (just the statement numbers) are true.\n","\n"," **Q1.1 = Statements 6, 7, and 9 are always correct.**\n","\n"," **Q1.2 = Statements 3 and 8 are always correct.**"]},{"cell_type":"markdown","metadata":{"id":"Qr835oX2Q4d4"},"source":["# Q2. N-gram language model (3)\n","\n","Create a 5-gram language model trained on the Shakespeare dataset. Store the relevant probabilities P(word|context) in python dictionaries. Do not use any smoothing or back-off (until Question 4). Pay special attention to beginning and end of sequences in the modeling process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VH7dZGzQQ7X1"},"outputs":[],"source":["import requests\n","import collections\n","import random\n","import sys\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import math\n","from math import log\n","from math import exp\n","import copy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TvlToTkkQ_mx","outputId":"ab41b15e-5e88-4c1d-8b16-c9c562a1d4e2","executionInfo":{"status":"ok","timestamp":1706838520896,"user_tz":480,"elapsed":40,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1115394,\n"," 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou')"]},"metadata":{},"execution_count":40}],"source":["url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n","response = requests.get(url)\n","response.raise_for_status() # Raise an exception for invalid HTTP status codes\n","text_data = response.text\n","len(text_data), text_data[:100]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qrhtxdpuW3OT","outputId":"56911c7a-3691-4a11-f157-816d46739a1e","executionInfo":{"status":"ok","timestamp":1706838520897,"user_tz":480,"elapsed":37,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["ow'd;' the Romans,\n","'This we received;' and each in either side\n","Give the all-hail to thee and cry 'Be\n"]}],"source":["# sample\n","pos = random.randint(0, len(text_data) - 1000)\n","print(text_data[pos:pos + 100])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2WXecoY-jsHl","outputId":"d88e04b8-0c37-4f70-e6ad-83af136c8fcb","executionInfo":{"status":"ok","timestamp":1706838520898,"user_tz":480,"elapsed":33,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1451204,\n"," '<s> First Citizen: </s> <s> Before we proceed any further , hear me speak .  </s> <s> All: </s> <s> ')"]},"metadata":{},"execution_count":42}],"source":["# preprocessing - do not change\n","text_data = response.text\n","text_data = text_data.replace(',', ' , ').replace('.', ' . ').replace('?', ' ? ').replace('!', ' ! ')\n","text_data = text_data.replace('  ', ' ')\n","text_data = text_data.replace('\\n\\n', '\\n').replace('\\n', ' </s> <s> ')\n","text_data = '<s> ' + text_data + ' </s>'\n","len(text_data), text_data[:100]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88hK3UwGhLPt","outputId":"3b94b2f6-eb1c-4068-b6e6-fad0e89772a1","executionInfo":{"status":"ok","timestamp":1706838520899,"user_tz":480,"elapsed":30,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1441204, 10000)"]},"metadata":{},"execution_count":43}],"source":["# train-test dataset split:\n","test_data = text_data[-10_000:]\n","text_data = text_data[:-10_000]\n","len(text_data), len(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZXQmnhksMm1","outputId":"a1dabb15-aae7-41f4-b11b-85f77180e3da","executionInfo":{"status":"ok","timestamp":1706838520899,"user_tz":480,"elapsed":26,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(314074,\n"," ['<s>',\n","  'First',\n","  'Citizen:',\n","  '</s>',\n","  '<s>',\n","  'Before',\n","  'we',\n","  'proceed',\n","  'any',\n","  'further',\n","  ',',\n","  'hear',\n","  'me',\n","  'speak',\n","  '.',\n","  '',\n","  '</s>',\n","  '<s>',\n","  'All:',\n","  '</s>'])"]},"metadata":{},"execution_count":44}],"source":["# split words\n","data = text_data.split(' ')\n","len(data), data[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DpsE6TO2QJXe","outputId":"9ed2e56d-b09e-4601-8766-ca76ad26db64","executionInfo":{"status":"ok","timestamp":1706838520900,"user_tz":480,"elapsed":23,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["one , or little .  </s> <s> GONZALO: </s> <s> How and lusty the grass looks ! how green !  </s> <s> \n"]}],"source":["# UPDATE: Removing unseen tokens from test_data\n","vocab = set(data)\n","test_data = ' '.join([_ for _ in test_data.split(' ') if _ in vocab]) # Adding unknown tokens to the test dataset from the training dataset.\n","print(test_data[:100])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L8iunWOah5QR"},"outputs":[],"source":["five = collections.defaultdict(lambda:0)\n","four = collections.defaultdict(lambda:0)\n","tri = collections.defaultdict(lambda:0)\n","bi = collections.defaultdict(lambda:0)\n","uni = collections.defaultdict(lambda:0)\n","\n","#INSERT CODE HERE\n","def ngram_extract(data, n): # Arguments: data -> training dataset; n -> number of n-grams.\n","  n_grams = []\n","  dict_ngram = {}\n","\n","  for iter in range(len(data) - n + 1):\n","    n_grams.append(tuple(data[iter:iter + n]))\n","\n","  for grams in n_grams:\n","    dict_ngram[grams] = dict_ngram.get(grams, 0) + 1\n","\n","  return dict_ngram\n","\n","\n","def probability(dict_ngram, previous_ngram, dict_probabilities, n): # Arguments: dict_ngram -> keys- n-grams and values- count; previous_ngram -> words excluding the final word of the n-gram;  dict_probabilities -> returns the probabilites (values) and n-grams (keys).\n","  # Unigram case:\n","  if n == 1:\n","\n","    for ele, val in dict_ngram.items():\n","      denom = sum(list(dict_ngram.values()))\n","\n","      if denom != 0:\n","          dict_probabilities[ele[0]] = val / denom\n","      else:\n","          dict_probabilities[ele[0]] = 0\n","\n","  else: # Other n-gram cases:\n","\n","    for ele, val in dict_ngram.items():\n","      count_lower = previous_ngram.get(ele[:n - 1], 0)\n","\n","      if count_lower != 0:\n","          dict_probabilities[ele] = val / count_lower\n","      else:\n","          dict_probabilities[ele] = 0\n","\n","  return dict_probabilities"]},{"cell_type":"code","source":["# Function call:\n","uni_gram = ngram_extract(data, 1)\n","bi_gram = ngram_extract(data, 2)\n","tri_gram = ngram_extract(data, 3)\n","four_gram = ngram_extract(data, 4)\n","five_gram = ngram_extract(data, 5)\n","\n","uni = probability(uni_gram, uni_gram, uni, 1)\n","bi = probability(bi_gram, uni_gram, bi, 2)\n","tri = probability(tri_gram, bi_gram, tri, 3)\n","four = probability(four_gram, tri_gram, four, 4)\n","five = probability(five_gram, four_gram, five, 5)"],"metadata":{"id":"j4E43cM2WFBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j54tdjyD-3f9"},"outputs":[],"source":["# Evaluation - Do not change\n","assert five[('Before', 'we', 'proceed', 'any', 'further')] == 1.0 # prob of last given prev 4\n","assert four[('<s>', 'First', 'Citizen:', '</s>')] == 1.0 # prob of last given prev 3\n","assert tri[('<s>', 'First', 'Citizen:')] == 0.172 # prob of last given prev 2\n","assert round(bi[('First', 'Citizen:')], 3) == 0.169 # prob of last given prev 1\n","assert round(uni[('Citizen:')], 5) == 0.00031 # prob of last"]},{"cell_type":"markdown","metadata":{"id":"gOuU211BCR2T"},"source":["Using the dictionaries containing unigram, bigram, ... five-gram probabilities, return probability P(word|context) as a single scalar. Here context is a list of previous tokens, e.g., [`You`, `may`, `do`, `as`, `you`] and word is a single potential next word, e.g., `like`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TkThhB-G1Pnq"},"outputs":[],"source":["def get_prob_from_lm(context = [], word = \"\"):\n","\n","  if len(context) == 0: # Unigram\n","    grams = copy.deepcopy(context)\n","    grams.append(word)\n","    return uni[tuple(grams)[0]]\n","\n","  elif len(context) == 1: # Bigram\n","    grams = copy.deepcopy(context)\n","    grams.append(word)\n","    return bi[tuple(grams)]\n","\n","  elif len(context) == 2: # Trigram\n","    grams = copy.deepcopy(context)\n","    grams.append(word)\n","    return tri[tuple(grams)]\n","\n","  elif len(context) == 3: # Fourgram\n","    grams = copy.deepcopy(context)\n","    grams.append(word)\n","    return four[tuple(grams)]\n","\n","  elif len(context) == 4: # Fivegram\n","    grams = copy.deepcopy(context)\n","    grams.append(word)\n","    return five[tuple(grams)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5L1wU1LnKpRC","outputId":"2c7ee98f-8a89-44db-ca6a-a779fdbf8a49","executionInfo":{"status":"ok","timestamp":1706838528083,"user_tz":480,"elapsed":241,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.172"]},"metadata":{},"execution_count":50}],"source":["get_prob_from_lm([\"<s>\", \"First\"], \"Citizen:\")"]},{"cell_type":"markdown","metadata":{"id":"RNG3-Fx5_-DJ"},"source":["# Q3. Evaluate Perplexity (2.5)\n","\n","Now let's evaluate the models quantitively using the intrinsic metric **perplexity**.\n","\n","Recall perplexity is the inverse probability of the test text\n","$$\\text{ppl}(w_1, \\dots, w_n) = p(w_1, \\dots, w_n)^{-\\frac{1}{N}}$$\n","\n","For an n-gram model, perplexity is computed by\n","$$\\text{ppl}(w_1, \\dots, w_n) = (\\prod_i p(w_{i+n}|w_i^{i+n-1})^{-\\frac{1}{N}}$$\n","\n","To get rid of numerical issue, we usually compute through:\n","$$\\text{ppl}(w_1, \\dots, w_n) = \\exp(-\\frac{1}{N}\\sum_i \\log p(w_{i+n}|w_i^{i+n-1}))$$\n","\n","\n","*Input:*\n","\n","+ **prob_function**: the `get_prob_from_lm` you implemented in Q2.\n","+ **data**: test data is a string of text containing multiple words.\n","\n","*Output:*\n","+ the perplexity of given data under a language model defined by **prob_function**\n","\n","Note that you do NOT need to optimize the language model in any way so as to minimize perplexity. Your reported perplexity will have no correlation with your score on this assignment, as far as it is implemented correctly."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZmH82w2LDA1s"},"outputs":[],"source":["def compute_perplexity(prob_function = get_prob_from_lm, data = test_data):\n","  ...\n","  # Hint: something = prob_function(context, word)\n","  # INSERT CODE HERE\n","  p = 0\n","  list_word = data.split()\n","  N = len(list_word)\n","\n","  for words in (range(N)):\n","    word = list_word[words]\n","    context = list_word[max(0, words - 4):words]\n","    p += log(prob_function(context, word))\n","\n","  ppl = exp(-p / N)\n","  return ppl"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VjXSmCU1DXCJ","outputId":"66e666f3-3d4c-4acf-af62-0d53d6274304","executionInfo":{"status":"ok","timestamp":1706838528085,"user_tz":480,"elapsed":222,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3.692430456031934"]},"metadata":{},"execution_count":52}],"source":["simple_data = 'Before we proceed any further , hear me speak .'\n","compute_perplexity(get_prob_from_lm, simple_data)"]},{"cell_type":"markdown","metadata":{"id":"ZsfSFUf-u3BT"},"source":["Can the perplexity of a sentence under any 5-gram model be 1? Explain.\n","\n","**\\# INSERT TEXT HERE**\n","\n","No, the perplexity of a sentence under nay 5-gram model can not be 1. The perplexity of the model depends on the probability values. The perplexity can be equal to 1 only if the probability is 1, which means that this sentence has apperared both in the training and the testing dataset (this is a very rare case). Having said this, it is possible for common sentences like, Good morning ! Hello neighbor, etc. to appear both in the training and testing dataset. In such an instace, the perplexity could be equal to one."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"fuZb0Zo5Ddft","outputId":"6b42aa27-0728-4499-8e8c-f0f81798149b","executionInfo":{"status":"error","timestamp":1706838528086,"user_tz":480,"elapsed":119,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"math domain error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-8803f8599af9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-51-ed55836f7358>\u001b[0m in \u001b[0;36mcompute_perplexity\u001b[0;34m(prob_function, data)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mppl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: math domain error"]}],"source":["compute_perplexity(data = test_data)"]},{"cell_type":"markdown","metadata":{"id":"1_X3zkY3V4_d"},"source":["**Explain** the resulting perplexity above on test data.\n","Conjecture (in text, not code) how it could be improved?\n","\n","The perplexity for the above case could not be computed as the text does not appear in the training dataset and hence has resulted in a zero probability value. Smoothing techniques (add-1 smoothing) can abvoid this problem.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wUlraNm_jKZk"},"source":["# Q4. Interpolation Smoothing (1)\n","\n","Using the dictionaries containing unigram, bigram, ... five-gram probabilities, return probability P(word|context) where context is the previous string, e.g., `You may do as you` and word is a single potential next word, e.g., `like`.\n","\n","Unlike in Q2, this time use interpolation smoothing as discussed in class."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QypKY49tzwMP","colab":{"base_uri":"https://localhost:8080/"},"outputId":"43f11843-998b-42d2-8fba-5e12bd3eaad3","executionInfo":{"status":"ok","timestamp":1706838537603,"user_tz":480,"elapsed":304,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.603281246928514"]},"metadata":{},"execution_count":54}],"source":["l1 = 0.2; l2 = 0.2; l3 = 0.2; l4 = 0.2; l5 = 0.2\n","# lambdas needed for Interpolation - you do NOT need to change these for the assignment\n","def get_prob_from_lm_with_interpolation(context = [], word = \"\"):\n","  # INSERT CODE HERE\n","  context_1 = []\n","  context_2 = context[-1:]\n","  context_3 = context[-2:]\n","  context_4 = context[-3:]\n","  context_5 = context[-4:]\n","\n","  p = l1 * get_prob_from_lm(context_1, word) + l2 * get_prob_from_lm(context_2, word) + l3 * get_prob_from_lm(context_3, word) + l4 * get_prob_from_lm(context_4, word) + l5 * get_prob_from_lm(context_5, word)\n","\n","  return p\n","\n","get_prob_from_lm_with_interpolation(['Before', 'we', 'proceed', 'any'], 'further')"]},{"cell_type":"markdown","metadata":{"id":"V3VKjyhyEVcC"},"source":["Now we re-evaluate perplexity with the new probability estimates (interpolated)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h86ON9wtEQ6u","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3d270289-c923-4904-a03c-22cbcfb1d829","executionInfo":{"status":"ok","timestamp":1706838540445,"user_tz":480,"elapsed":272,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["6.245056223146501"]},"metadata":{},"execution_count":55}],"source":["compute_perplexity(prob_function = get_prob_from_lm_with_interpolation, data = simple_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VYLLgaYgERTe","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2cb6bf52-3f7a-40b2-c994-0117c24b8ce3","executionInfo":{"status":"ok","timestamp":1706838542656,"user_tz":480,"elapsed":245,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["213.15016423735972"]},"metadata":{},"execution_count":56}],"source":["compute_perplexity(prob_function = get_prob_from_lm_with_interpolation, data = test_data)"]},{"cell_type":"markdown","metadata":{"id":"UscwpcjrtHf5"},"source":["# Q5. Text Generation (2.5)\n","\n","Our final goal is to generate/infer/sample from the language model that we have created.\n","Given a prefix or prompt, the model can generate text according to estimated next word probabilities (similar to any LLM, e.g., GPT).\n","\n","Complete the following function using the dictionaries already available to sample (you may use `random.choice` function) from the distribution of possible next tokens. Remember to randomly sample (from a weighted distribution over all possible next words in the vocabulary) instead of picking only the most likely next word.\n","\n","Input:\n","- dictionaries `uni`, `bi`, `tri`, `four`, `five`\n","- **n_generate**: number of maximum words to infer. E.g., `2`\n","- **prefix**: the context on which to condition text generation. E.g., `as you`\n","\n","Output: string output containing prefix and generated words separated by whitespace. E.g., `as you like it`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FUtpND2XtH11","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"e3c49e15-6234-4bc4-f248-764bf2652939","executionInfo":{"status":"ok","timestamp":1706838552155,"user_tz":480,"elapsed":6248,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"as you say you </s> <s> Where you'll <s> To With </s> You ELIZABETH: indeed , thou but the elements that turtle ! thank\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":57}],"source":["prefix = 'as you say you' # \"hear me\"\n","\n","def generate(prob_fn = get_prob_from_lm_with_interpolation, prefix = prefix, n_generate = 20):\n","  next_prediction = prefix.split()\n","\n","  for _ in range(n_generate):\n","        predict_probability = {}\n","        context = next_prediction[-4:]\n","\n","        for sent in uni.keys():\n","            p = prob_fn(context, sent)\n","            predict_probability[sent] = p\n","\n","        next_prediction.append(random.choices(list(predict_probability.keys()), weights = list(predict_probability.values()))[0])\n","\n","  return ' '.join(next_prediction)\n","\n","generate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJJweT7Gxgkv","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"3ef8f4f8-fc23-4074-8037-b6de60092349","executionInfo":{"status":"ok","timestamp":1706838582874,"user_tz":480,"elapsed":1622,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'as you say you </s> <s> have none .  </s> <s> Young Ned , for thee , thine uncles and myself </s> <s>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":62}],"source":["generate(prob_fn = get_prob_from_lm)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdEGwdhN0t_Z","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"412f80d4-5173-48a9-80fa-070a19b77a6a","executionInfo":{"status":"ok","timestamp":1706838591831,"user_tz":480,"elapsed":5884,"user":{"displayName":"Sudarshana Sudheendra Rao","userId":"10427931692310872463"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"as you say you ,  </s> <s> Being capable grant , of Stanley ? what have </s> <s> I'ld with thee with that\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":63}],"source":["generate(prob_fn = get_prob_from_lm_with_interpolation)"]},{"cell_type":"markdown","metadata":{"id":"gZzRMpNBxjKN"},"source":["Compare the outputs of the two language models. Why do you think a difference exists?\n","\n","**\\# INSERT TEXT HERE**\n","\n","The output text generated by the model without the interpolation has resulted in a more meaningful sentence than the model which employs interpolation. This could be due to the fact that the probability weights are not tuned. This behavior is expected as thew perplexity of the model with interpolation is worse than the model without interpolation."]},{"cell_type":"markdown","metadata":{"id":"yTw020PNa5_Z"},"source":["# The End\n","Voila! If you were able to complete the assignment, you have successfully created a language model in barebones Python! You have trained it on a text corpus and can use it to generate arbitrary text just like GPT-3!"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}